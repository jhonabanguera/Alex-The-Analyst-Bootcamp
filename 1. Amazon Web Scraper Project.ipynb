{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55776e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import smtplib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27269d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1970249965.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install lxml\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Lxml I used to try another way to scrap the data\n",
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb91285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product Title =  NA\n",
      "product Price =  NA\n"
     ]
    }
   ],
   "source": [
    "# Another try using lxml\n",
    "\n",
    "URL = 'https://www.amazon.com/dp/B0B3BVWJ6Y/'\n",
    "HEADERS = ({\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 44.0.2403.157 Safari / 537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'})\n",
    " \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    " \n",
    "product_name = ''\n",
    "product_price = ''\n",
    "try:\n",
    "    product_title = soup.find(\"span\",\n",
    "                              attrs={\"id\": 'productTitle'})\n",
    "    product_name = product_title.string.strip().replace(',', '')\n",
    " \n",
    "except AttributeError:\n",
    "    product_name = \"NA\"\n",
    " \n",
    "try:\n",
    "    product_price = soup.find(\"span\", attrs={'class': 'a-offscreen'}).string.strip().replace(',', '')\n",
    "except AttributeError:\n",
    "    product_price = \"NA\"\n",
    " \n",
    "print(\"product Title = \", product_name)\n",
    "print(\"product Price = \", product_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f986b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title not found\n",
      "Price not found\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website and pull in data\n",
    "\n",
    "URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data%2Banalyst%2Btshirt&qid=1626655184&sr=8-3&customId=B0752XJYNL&th=1'\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(soup.prettify(), \"html.parser\")\n",
    "\n",
    "#fIND THIS METHOD BECAUSE THE WEBSITE WAS NOT LETTING ME PULL OUT THE DATA\n",
    "title_element = soup2.find(id='productTitle')\n",
    "price_element = soup2.find(id='priceblock_ourprice')\n",
    "\n",
    "title = title_element.get_text(strip=True) if title_element else 'Title not found'\n",
    "price = price_element.get_text(strip=True) if price_element else 'Price not found'\n",
    "\n",
    "print(title)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1f9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price not found\n",
      "Title not found\n"
     ]
    }
   ],
   "source": [
    "price = price.strip()[:]\n",
    "title = title.strip()\n",
    "\n",
    "print(price)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1ab12f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-01\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc519f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "header = ['Title', 'Price', 'Date']\n",
    "data = [title, price, today]\n",
    "\n",
    "\n",
    "#with open('AmazonWebScraperDataset.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerow(header)\n",
    "#    writer.writerow(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41eb96dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Title            Price        Date\n",
      "0   Title not found  Price not found  2023-07-01\n",
      "1   Title not found  Price not found  2023-07-01\n",
      "2   Title not found  Price not found  2023-07-01\n",
      "3   Title not found  Price not found  2023-07-01\n",
      "4   Title not found  Price not found  2023-07-01\n",
      "5   Title not found  Price not found  2023-07-01\n",
      "6   Title not found  Price not found  2023-07-01\n",
      "7   Title not found  Price not found  2023-07-01\n",
      "8   Title not found  Price not found  2023-07-01\n",
      "9   Title not found  Price not found  2023-07-01\n",
      "10  Title not found  Price not found  2023-07-01\n",
      "11  Title not found  Price not found  2023-07-01\n",
      "12  Title not found  Price not found  2023-07-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\user\\AmazonWebScraperDataset.csv')\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2efc34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are appending data to the CSV file\n",
    "\n",
    "with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33e66645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate the process so it does it in the background\n",
    "\n",
    "def check_price():\n",
    "    URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data%2Banalyst%2Btshirt&qid=1626655184&sr=8-3&customId=B0752XJYNL&th=1'\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    soup2 = BeautifulSoup(soup.prettify(), \"html.parser\")\n",
    "\n",
    "    #fIND THIS METHOD BECAUSE THE WEBSITE WAS NOT LETTING ME PULL OUT THE DATA\n",
    "    title_element = soup2.find(id='productTitle')\n",
    "    price_element = soup2.find(id='priceblock_ourprice')\n",
    "    \n",
    "    title = title_element.get_text(strip=True) if title_element else 'Title not found'\n",
    "    price = price_element.get_text(strip=True) if price_element else 'Price not found'\n",
    "\n",
    "    \n",
    "    import datetime\n",
    "\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    print(today)\n",
    "    \n",
    "    import csv \n",
    "\n",
    "    header = ['Title', 'Price', 'Date']\n",
    "    data = [title, price, today]\n",
    "    \n",
    "    with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    \n",
    "    #Example of what you could do with these kind of projects\n",
    "    #if(price < 14):\n",
    "    #   send_email()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43496fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-01\n"
     ]
    }
   ],
   "source": [
    "#Check the data every day\n",
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and using pandas to read the csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\user\\AmazonWebScraperDataset.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5b875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
